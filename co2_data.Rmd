---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
ts_data <- co2
```


```{r}
plot(ts_data, xlab = "Year",ylab = "co2 concentration")
```


```{r}
#TASK 2: DECOMPOSE THE TIME SERIES
#1. Perform additive decomposition of the time series.
add_decompose <- decompose(ts_data,type = "additive")
#2. Plot the components (trend, seasonal, and random).
plot(add_decompose)
```


```{r}
#3. Perform multiplicative decomposition of the time series.
mul_decompose <- decompose(ts_data, type = "multiplicative")
#4. Plot the components (trend, seasonal, and random).
plot(mul_decompose)
```


```{r}
#5. Compare the results of additive and multiplicative decomposition:
```


```{r}
#TASK 3: RESIDUAL ANALYSIS
#1. Extract the residuals from the additive decomposition.
residual_add <- add_decompose$random
residual_mul <- mul_decompose$random
#Plot the residuals and examine their behavior.
plot(residual_add, main = "residuals from the additive decomposition")

#3. Are the residuals stationary (constant mean and variance)? Provide your reasoning.
#H0: Not Stationary
#H1: Stationary
library(tseries)
adf_test1 <- adf.test(na.omit(residual_add))
adf_test2 <- adf.test(na.omit(residual_mul))
print(adf_test1)
print(adf_test2)
#The series is stationary as p value < 0.05, Accepting the Alternate Hypothesis that series is stationary
```


```{r}
#TASK 4: TEST FOR STATIONARITY
#1. Perform a unit root test (e.g., Augmented Dickey-Fuller test) on the original time series.
adf_test_original <- adf.test(ts_data)
print(adf_test_original)
#p value > 0.05, implies that we accept the NULL Hypothesis
```


```{r}
#TASK 5: DIFFERENCING
#1. Apply first-order differencing to the time series and plot the differenced series.
diff_ts1 <- diff(ts_data,differences = 1)
plot(diff_ts1, main = "First order difference")

#2. Apply seasonal differencing (if applicable) and plot the seasonally differenced series.
seasonal_diff_ts <- diff(ts_data, lag = 12)
plot(seasonal_diff_ts, main = "Seasonal Difference")

#3. Test the differenced series for stationarity using the unit root test.Comment on the results.
#ADF Test on differenced series
adf_test_diff1 <- adf.test(na.omit(diff_ts1))
adf_test_season1 <- adf.test(na.omit(seasonal_diff_ts))
print(adf_test_diff1)
print(adf_test_season1)
```


```{r}
#TASK 6: MODEL IDENTIFICATION
#1. Plot the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) of the differenced series.
acf(diff_ts1,main = "ACF of difference 1")
pacf(diff_ts1,main = "PACF of difference 1")
#lag = 1/12 for a month, lag 1 = 12 months 
acf(seasonal_diff_ts,main = "ACF of seasonal difference")
pacf(seasonal_diff_ts,main = "PACF of seasonal difference")
#help("acf")
```


```{r}
#Task 7: FIT ARIMA MODELS
#1. Fit an ARIMA model manually using the order identified in Task 6.
help("arima.sim")
arima1 <- arima(ts_data, order = c(1,1,1))
arima1
```


```{r}
#Use the auto.arima() function to select the best model automatically.
library(forecast)
best_arima_model <- auto.arima(ts_data)
print("Best ARIMA model")
print(best_arima_model)
#This means that ARIMA (AR(1),D(1), MA(1)) and SARIMA (Seasonal AR(1),Seasonal diff(1),
#Seasonal MA(2)) with a lag of 12.

#AIC (Akaike Information Criterion): A measure of model quality; lower values are better
#AICc: A corrected version of AIC for small sample sizes
#BIC (Bayesian Information Criterion): Penalizes model complexity more than AIC; lower values are better
```


```{r}
checkresiduals(best_arima_model)
```


```{r}
#3. Compare the manually fitted model with the auto-selected model:
#(a) Which model has better fit statistics (e.g., AIC, BIC)?
aic_man <- AIC(arima1)
bic_man <- BIC(arima1)
cat("AIC for Manual ARIMA is",aic_man)
cat("\nBIC for Manual ARIMA is",bic_man)
aic_auto <- AIC(best_arima_model)
bic_auto <- BIC(best_arima_model)
cat("\n\nAIC for Auto ARIMA is",aic_auto)
cat("\nBIC for Auto ARIMA is",bic_auto)
cat("\n\nWe can see that the AIC and BIC values generated by auto arima are less and are hence the best fit")
```


```{r}
#(b) How do the parameters differ between the two models?
#The parameters differ a lot between the ones generated by Auto ARIMA and manual ARIMA. The AIC and BIC values are much much lesser for Auto ARIMA.
```


```{r}
#TASK 8: SARIMA FOR SEASONAL DATA
#1. Fit a SARIMA model to the time series, including seasonal terms.
sarima <- arima(ts_data, order = c(1,1,1), seasonal = list(order = c(1,1,1), period = 12))
summary(sarima)
```


```{r}
#TASK 9: MODEL DIAGNOSTICS
#1. Perform residual diagnostics for the selected ARIMA or SARIMA model:
#(a) Plot the residuals and check for patterns.
#Plotting SARIMA
ts.plot(sarima$residuals,
        main = "Residuals of the fitted SARIMA model",
        ylab = "Residuals",
        col = "blue")
#Plotting ARIMA
ts.plot(arima1$residuals,
        main = "Residuals of the manually fitted ARIMA model",
        ylab = "Residuals",
        col = "red")
```


```{r}
#(b) Conduct a Ljung-Box test to assess residual independence.
Box.test(sarima$residuals, lag = 12, type = "Ljung-Box")
#H0: Residuals are uncorrelated
#H1: Residuals are correlated
#the p value (at lag = 12) is greater than 0.05 which implies that we accept the NULL Hypothesis. This implies that the residuals are uncorrelated or independent.

#Just for information, if p < 0.05, implies that residuals show significant autocorrelation, suggesting the model could be improved.
```


```{r}
#2. Are the residuals white noise? Justify your answer.

ts.plot(sarima$residuals,main = "Residuals of fitted SARIMA model",ylab="residuals")
acf(sarima$residuals,main="ACF of SARIMA")

#Conclusion: The residual is a white noise. Following are the plausible reasons for the same:
#ACF plot lies within the confidence interval, also there are no spikes in the ACF plot implies that there is no Auto-Correlation factor present.
#As per the output from Ljung test, the p value is greater than 0.05 i.e. 0.4714. This implies that the residuals are independent and confirms that residual is a White Noise.
```


```{r}
#TASK 10; FORECASTING
#1. Generate forecasts for the next 12 months using the best-selected model.
library(forecast)
forecasted_sarima <- forecast(sarima, h=12)
forecasted_sarima24 <- forecast(sarima, h=24)
forecasted_sarima36 <- forecast(sarima, h=36)
forecasted_sarima60 <- forecast(sarima, h=60)
forecasted_sarima1 <- forecast(sarima)
print(forecasted_sarima)
```


```{r}
#2. Plot the forecasts along with the original time series.
ts.plot(ts_data, main = "Original Time Series")
ts.plot(forecasted_sarima, main="Forecasted SARIMA h=12")
ts.plot(forecasted_sarima24, main="Forecasted SARIMA h=24")
ts.plot(forecasted_sarima36, main="Forecasted SARIMA h=36")
ts.plot(forecasted_sarima60, main="Forecasted SARIMA h=60")
ts.plot(forecasted_sarima1, main="Forecasted SARIMA w/o h=12")
```


```{r}
#3. Interpret the forecast results:

#(a) How well do the forecasts capture the trends and seasonality?
#The forecasting perfectly captures the trend and seasonality. For a better qualilty for decision making, h=60 was kept. The forecasted pattern strictly follows "Homoskedasticity" as like for the original dataset and follows the same growing pattern.

#(b) What are the potential limitations of your forecast?
#SARIMA model is based on fixed seasonality, in case if the seasonality gets disrupted, the model will fail.
#SARIMA model does not account any exogenous effect therefore in case of any external event, the model will fail.


